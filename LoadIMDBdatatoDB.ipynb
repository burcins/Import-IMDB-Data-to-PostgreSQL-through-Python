{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing IMDB DATASETS to PostgreSQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I prepared a pipeline to dump IMDB data to Postgres DB through Python. But as a requirement it is needed to install Postgres to your local machine and create a DB as well as a user to upload all datasets as tables. This notebook does not cover this part. \n",
    "\n",
    "Of course, the datasets can be imported directly to Postgres by using ***CREATE TABLE*** and ***COPY*** commands or just clicking from available menus through pgAdmin IDE. But the main idea in here is to split them into chunks and manupilate datasets before dumping them. \n",
    "\n",
    "After creation of DB and extracting downloaded files from IMDB data page, you can use this notebook to create tables and load data into that tables separately. The main drawback of IMDB dataset is its volume. So I used pandas in chunks for reading some of the tables and used Dask for some others to show both options. \n",
    "\n",
    "I downloaded publicly published IMDB dataset from this [link](https://datasets.imdbws.com/), in which includes 7 different datasets as tsv files. Explanalations of the data can be investigated through this [page](https://www.imdb.com/interfaces/). \n",
    "\n",
    "*** Please do not forget to change FILE DIRECTORY, DB NAME, USERNAME AND PASSWORD with your own ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import csv, os, io\n",
    "from io import StringIO\n",
    "from sqlalchemy import create_engine\n",
    "from dask import dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname= <your DB name> user=<your username>  password=<your password> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL database version:\n",
      "('PostgreSQL 12.8 (Ubuntu 12.8-0ubuntu0.20.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "        \n",
    "# execute a statement\n",
    "print('PostgreSQL database version:')\n",
    "cur.execute('SELECT version()')\n",
    "\n",
    "# display the PostgreSQL database server version\n",
    "db_version = cur.fetchone()\n",
    "print(db_version)\n",
    "\n",
    "# close the communication with the PostgreSQL\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  I got this function from this webpage to solve to upload big data problem \n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql-method\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titlecrew has number of : (8285162,)\n",
      "titleepisode has number of : (6079951,)\n",
      "namebasics has number of : (11238430,)\n",
      "titleratings has number of : (1187847,)\n",
      "titlebasics has number of : (8285162,)\n"
     ]
    }
   ],
   "source": [
    "for datafile in [x for x in os.listdir('/home/burcin/Downloads') if x.endswith('tsv') and x not in ['titleakas.tsv', 'titleprincipals.tsv']]:\n",
    "    dataname = datafile.split('.')[0]\n",
    "    # read data\n",
    "    df = pd.read_csv('~/Downloads/'+datafile, sep= '\\t',chunksize=10000)\n",
    "    pd_df = pd.concat(df)\n",
    "    pd_df = pd_df.replace( '\\\\N', '')\n",
    "    # load data to DB\n",
    "    conn = psycopg2.connect(\"dbname= <your DB name> user=<your username>  password=<your password> \")\n",
    "    engine = create_engine('postgresql://<your username>:<your password>@localhost:5432/<your DB name')\n",
    "    pd_df.to_sql(dataname, engine, method=psql_insert_copy)\n",
    "    #control operation\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "    select count(*) from \"\"\"+dataname+\"\"\"\n",
    "                \"\"\")\n",
    "\n",
    "    datacount = cur.fetchone()\n",
    "    print(dataname, 'has number of rows:' , datacount)\n",
    "\n",
    "    cur.close()\n",
    "    \n",
    "    # delete dataframe to lighten memory\n",
    "    del pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titleprincipals has number of rows: (46886580,)\n",
      "titleakas has number of rows: (29186753,)\n"
     ]
    }
   ],
   "source": [
    "for datafile in [x for x in os.listdir('/home/burcin/Downloads') if x.endswith('tsv') and x in ['titleakas.tsv', 'titleprincipals.tsv']]:\n",
    "    dataname = datafile.split('.')[0]\n",
    "    # read data\n",
    "    \n",
    "    if datafile == 'titleakas.tsv':\n",
    "        dask_df = dd.read_csv('~/Downloads/'+ datafile, sep = '\\t', dtype={'isOriginalTitle': 'object'})\n",
    "        dask_df = dask_df.replace( '\\\\N', '')\n",
    "    else :\n",
    "        dask_df = dd.read_csv('~/Downloads/'+ datafile, sep = '\\t')\n",
    "        dask_df = dask_df.replace( '\\\\N', '')        \n",
    "\n",
    "    \n",
    "    # create empty table in DB\n",
    "    \n",
    "    engine = create_engine('postgresql://<your username>:<your password>@localhost:5432/<your DB name')\n",
    "    \n",
    "    pd.DataFrame(columns=dask_df.columns).to_sql(\n",
    "        dataname, \n",
    "        con=engine, \n",
    "        if_exists='replace', \n",
    "        index=False)\n",
    "\n",
    "    \n",
    "    # load data to DB\n",
    "    cur = conn.cursor()\n",
    "    for n in range(dask_df.npartitions):    \n",
    "        table_chunk = dask_df.get_partition(n).compute()\n",
    "        output = io.StringIO()\n",
    "        table_chunk.to_csv(output, sep='\\t', header=False, index=False)\n",
    "        output.seek(0)\n",
    "        try:\n",
    "            cur.copy_from(output, dataname, null='')\n",
    "        except Exception:\n",
    "            err_tables.append(table_chunk)\n",
    "            conn.rollback()\n",
    "            continue\n",
    "        conn.commit()\n",
    "\n",
    "        \n",
    "    # check if data loaded to DB    \n",
    "    cur.execute(\"\"\"\n",
    "    select count(*) from \"\"\"+dataname+\"\"\"\n",
    "                \"\"\")\n",
    "\n",
    "    datacount = cur.fetchone()\n",
    "    print(dataname, 'has number of rows:' , datacount)\n",
    "\n",
    "    cur.close()\n",
    "    \n",
    "    # delete dataframe to lighten memory\n",
    "    del dask_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
